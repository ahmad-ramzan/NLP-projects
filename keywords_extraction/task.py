# -*- coding: utf-8 -*-
"""practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19vZzEMB-fmTEcmrk5Fka98KSgGnuIcfb
"""

import os
os.environ['KAGGLE_USERNAME'] =
os.environ['KAGGLE_KEY'] =
!kaggle datasets download -d benhamner/nips-papers

!unzip /content/nips-papers.zip

import pandas as pd
df = pd.read_csv('/content/papers.csv')

df.head()

df['paper_text'][0]

"""##Check null values"""

df.isnull().sum()
df.isna().sum()

df = df.fillna('')

df.isnull().sum()

"""##Preprocessing of text"""

import re
import string
import nltk
# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))
from nltk.stem.porter import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

def preprocess_text(txt):
    # Lowercase
    txt = txt.lower()
    # Remove HTML tags (if any)
    txt = re.sub(r"<.*?>", " ", txt)
    # Remove URLs (if any)
    txt = re.sub(r"https?://\S+|www\.\S+", " ", txt)
    # Remove Punctuation
    txt = txt.translate(str.maketrans('', '', string.punctuation))
    # Remove numbers (optional, if you want to keep numbers, you can skip this step)
    txt = re.sub(r'[^A-Za-z\s]', '', txt)
    # Tokenize text
    words = word_tokenize(txt)
    # Remove words less than two letters
    words = [word for word in words if len(word) > 2]
    # Remove stopwords
    words = [word for word in words if word not in STOPWORDS]
    # Lemmatize
    lmt = WordNetLemmatizer()
    words = [lmt.lemmatize(word) for word in words]
    return " ".join(words)


# Sample text (your input text)
text = '''Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details'''

# Preprocess the text
cleaned_text = preprocess_text(text)

# Print the cleaned text
print("Cleaned Text:", cleaned_text)

df['paper_text'] = df['paper_text'].apply(preprocess_text)

df['paper_text'][0]

"""## Vectorization"""

from sklearn.feature_extraction.text import CountVectorizer
# Reduce max_features and adjust n-gram range
cv = CountVectorizer(max_features=6000, ngram_range=(1, 2))
# Create a vocabulary and word count vectors
word_count_vectors = cv.fit_transform(df['paper_text'])

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)
tfidf_transformer.fit(word_count_vectors)

import numpy as np

# Assuming `cv` is the CountVectorizer and `tfidf_transformer` is the TfidfTransformer

# Step 1: Get the TF-IDF scores for the document
tf_idf_vector = tfidf_transformer.transform(cv.transform([df['paper_text'][941]]))

# Step 2: Get the feature names (terms) from the CountVectorizer
feature_names = cv.get_feature_names_out()

# Step 3: Convert the sparse matrix to a dense array
dense_vector = tf_idf_vector.toarray()

# Step 4: Create a dictionary of terms and their corresponding TF-IDF scores
tf_idf_scores = {feature_names[i]: dense_vector[0][i] for i in range(len(feature_names))}

# Step 5: Sort the dictionary by TF-IDF score (in descending order)
sorted_tf_idf_scores = sorted(tf_idf_scores.items(), key=lambda x: x[1], reverse=True)

# Step 6: Display the top keywords with their TF-IDF scores
top_keywords = sorted_tf_idf_scores[:10]  # Get top 10 keywords
# print(top_keywords)
for keyword, score in top_keywords:
    print(f"{keyword}  =>  {score:.3f}")

"""## Model Training"""

import pickle
pickle.dump(tfidf_transformer,open('tfidf_transformer.pkl','wb'))
pickle.dump(cv,open('count_vectorizer.pkl','wb'))
# pickle.dump(feature_names,open('feature_names.pkl','wb'))

import pickle

tfidf_transformer = pickle.load(open('tfidf_transformer.pkl', 'rb'))
cv = pickle.load(open('count_vectorizer.pkl', 'rb'))
# feature_names = pickle.load(open('feature_names.pkl', 'rb'))

tf_idf_vector = tfidf_transformer.transform(cv.transform([df['paper_text'][941]]))

# Step 2: Get the feature names (terms) from the CountVectorizer
feature_names = cv.get_feature_names_out()

# Step 3: Convert the sparse matrix to a dense array
dense_vector = tf_idf_vector.toarray()

# Step 4: Create a dictionary of terms and their corresponding TF-IDF scores
tf_idf_scores = {feature_names[i]: dense_vector[0][i] for i in range(len(feature_names))}

# Step 5: Sort the dictionary by TF-IDF score (in descending order)
sorted_tf_idf_scores = sorted(tf_idf_scores.items(), key=lambda x: x[1], reverse=True)

# Step 6: Display the top keywords with their TF-IDF scores
top_keywords = sorted_tf_idf_scores[:10]  # Get top 10 keywords
# print(top_keywords)
for keyword, score in top_keywords:
    print(f"{keyword}  =>  {score:.3f}")