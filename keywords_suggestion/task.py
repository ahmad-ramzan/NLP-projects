# -*- coding: utf-8 -*-
"""practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19vZzEMB-fmTEcmrk5Fka98KSgGnuIcfb
"""

with open('autocorrect book.txt', 'r', encoding='utf-8') as f:
    data = f.read()

data

"""##Preprocessing of text"""

import re
import string
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize

def preprocess_text(txt):
    # Lowercase
    txt = txt.lower()
    # Remove HTML tags (if any)
    txt = re.sub(r"<.*?>", " ", txt)
    # Remove URLs (if any)
    txt = re.sub(r"https?://\S+|www\.\S+", " ", txt)
    # Remove Punctuation
    txt = txt.translate(str.maketrans('', '', string.punctuation))
    # Remove numbers (optional, if you want to keep numbers, you can skip this step)
    txt = re.sub(r'[^A-Za-z\s]', '', txt)
    # Tokenize text
    words = word_tokenize(txt)
    return words


# Sample text (your input text)
text = '''Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details'''

# Preprocess the text
cleaned_text = preprocess_text(text)

# Print the cleaned text
print("Cleaned Text:", cleaned_text)

cleaned_words = preprocess_text(data)

cleaned_words[:10]

set(cleaned_words[:10])

total_words = len(cleaned_words)

"""## Words Frequency"""

# find each word frequency
from collections import Counter
words_freq_dict = {}
words_freq_dict = Counter(cleaned_words)

# find relative frequency of each word
probs = {}

for k in words_freq_dict.keys():
    probs[k] = words_freq_dict[k] / total_words

for i, (word, prob) in enumerate(probs.items()):
    if i >= 10:
        break
    print(f"{word}: {prob:.5f}")

# find similar words
import difflib

def suggest_with_scores(word, probs, n=5):
    suggestions = []
    for w in probs:
        score = difflib.SequenceMatcher(None, word, w).ratio()
        if score > 0.7:
            suggestions.append((w, probs[w], score))

    # Sort by (probability * similarity score)
    sorted_suggestions = sorted(suggestions, key=lambda x: x[1] * x[2], reverse=True)
    return [w for w, _, _ in sorted_suggestions[:n]]

suggest_with_scores('him',probs)