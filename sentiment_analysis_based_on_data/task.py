# -*- coding: utf-8 -*-
"""practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19vZzEMB-fmTEcmrk5Fka98KSgGnuIcfb
"""

import os
os.environ['KAGGLE_USERNAME'] =
os.environ['KAGGLE_KEY'] =
!kaggle datasets download -d vishakhdapat/imdb-movie-reviews

!unzip /content/imdb-movie-reviews.zip

import pandas as pd
df = pd.read_csv('/content/IMDB Dataset.csv')

df.head()

df['review'][0]

"""##Check null values"""

df.isnull().sum()
df.isna().sum()

df = df.fillna('')

df.isnull().sum()

"""##Preprocessing of text"""

import re
import string
import nltk
# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))
from nltk.stem.porter import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

def preprocess_text(txt):
    # Lowercase
    txt = txt.lower()
    # Remove HTML tags (if any)
    txt = re.sub(r"<.*?>", " ", txt)
    # Remove URLs (if any)
    txt = re.sub(r"https?://\S+|www\.\S+", " ", txt)
    # Remove Punctuation
    txt = txt.translate(str.maketrans('', '', string.punctuation))
    # Remove numbers (optional, if you want to keep numbers, you can skip this step)
    txt = re.sub(r'[^A-Za-z\s]', '', txt)
    # Tokenize text
    words = word_tokenize(txt)
    # Remove words less than two letters
    words = [word for word in words if len(word) > 2]
    # Remove stopwords
    words = [word for word in words if word not in STOPWORDS]
    # Lemmatize
    lmt = WordNetLemmatizer()
    words = [lmt.lemmatize(word) for word in words]
    return " ".join(words)


# Sample text (your input text)
text = '''Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details'''

# Preprocess the text
cleaned_text = preprocess_text(text)

# Print the cleaned text
print("Cleaned Text:", cleaned_text)

df['review'] = df['review'].apply(preprocess_text)

df['review'][0]

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
le.fit(df['sentiment'])
df['sentiment'] = le.transform(df['sentiment'])

df['sentiment'].unique()

df['sentiment'][0]

"""## Vectorization"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english')

tfidf.fit(df['review'])
requiredText  = tfidf.transform(df['review'])

"""## Model Training"""

X = requiredText[:5000]
Y = df['sentiment'][:5000]

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, stratify=Y, random_state=1)
# Ensure that X_train and X_test are dense if they are sparse
X_train = X_train.toarray() if hasattr(X_train, 'toarray') else X_train
X_test = X_test.toarray() if hasattr(X_test, 'toarray') else X_test
model = LogisticRegression()
model.fit(X_train, Y_train)

#accuracy on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
print(f'Accuracy on train data : {training_data_accuracy:.4f}')

#accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)
print(f'Accuracy on test data : {test_data_accuracy:.4f}')

import pickle
pickle.dump(model,open('model.pkl','wb'))
pickle.dump(tfidf,open('tfidf.pkl','wb'))

import pickle

model = pickle.load(open('model.pkl', 'rb'))
tfidf = pickle.load(open('tfidf.pkl', 'rb'))

text = "one cerebr insight movi ever seen script languag costum sceneri plot charact etc suprem bore watch intens even listen movi work mani time lost count scarlett age grace acquir wisdom beyond year end movi take tara ireland famili origin see result anoth civil war play time irish english movi depict doubl standard men time man still respect girlfriend whore woman mere seen privat place man societi accus improprieti cours scarlett alway think outsid box break rule necessari creat need chang help peopl surviv scarlett beauti definit bone strength self esteem wisdom grow age rememb line confront much plate tomorrow anoth day way carri determin courag learn experi good movi show daughter teach woman import respect men especi handsom one two side may treat woman differ depend act respect attract woman need learn movi teach movi like therapi better gone wind second part take place ireland anyon irish decent cherish sceneri peopl scarlett charact within"

text = preprocess_text(text)
text  = tfidf.transform([text])
text = text.toarray() if hasattr(text, 'toarray') else text
pred = model.predict(text)

if pred == [1]:
    print("positive comment")
else:
    print("negative comment")